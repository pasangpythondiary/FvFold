{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QMoeBQnUCK_E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/pasang/.local/lib/python3.7/site-packages (1.12.1+cu116)\n",
            "Requirement already satisfied: transformers in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (4.24.0)\n",
            "Requirement already satisfied: sentencepiece in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (0.1.97)\n",
            "Requirement already satisfied: h5py in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (3.7.0)\n",
            "Requirement already satisfied: typing-extensions in /home/pasang/.local/lib/python3.7/site-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /home/pasang/.local/lib/python3.7/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/pasang/.local/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: filelock in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from transformers) (6.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/pasang/.local/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pasang/.local/lib/python3.7/site-packages (from requests->transformers) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pasang/.local/lib/python3.7/site-packages (from requests->transformers) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/pasang/.local/lib/python3.7/site-packages (from requests->transformers) (2023.5.7)\n"
          ]
        }
      ],
      "source": [
        "#@title Install requirements. { display-mode: \"form\" }\n",
        "# Install requirements\n",
        "!pip install torch transformers sentencepiece h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tRe7CfuqFFmY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘protT5’: File exists\n",
            "mkdir: cannot create directory ‘protT5/protT5_checkpoint’: File exists\n",
            "mkdir: cannot create directory ‘protT5/sec_struct_checkpoint’: File exists\n",
            "mkdir: cannot create directory ‘protT5/output’: File exists\n",
            "File ‘protT5/example_seqs.fasta’ already there; not retrieving.\n",
            "\n",
            "File ‘protT5/sec_struct_checkpoint/secstruct_checkpoint.pt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Set up working directories and download files/checkpoints. { display-mode: \"form\" }\n",
        "# Create directory for storing model weights (2.3GB) and example sequences.\n",
        "# Here we use the encoder-part of ProtT5-XL-U50 in half-precision (fp16) as \n",
        "# it performed best in our benchmarks (also outperforming ProtBERT-BFD).\n",
        "# Also download secondary structure prediction checkpoint to show annotation extraction from embeddings\n",
        "!mkdir protT5 # root directory for storing checkpoints, results etc\n",
        "!mkdir protT5/protT5_checkpoint # directory holding the ProtT5 checkpoint\n",
        "!mkdir protT5/sec_struct_checkpoint # directory storing the supervised classifier's checkpoint\n",
        "!mkdir protT5/output # directory for storing your embeddings & predictions\n",
        "!wget -nc -P protT5/ https://rostlab.org/~deepppi/example_seqs.fasta\n",
        "# Huge kudos to the bio_embeddings team here! We will integrate the new encoder, half-prec ProtT5 checkpoint soon\n",
        "!wget -nc -P protT5/sec_struct_checkpoint http://data.bioembeddings.com/public/embeddings/feature_models/t5/secstruct_checkpoint.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZQotVM94S7NR"
      },
      "outputs": [],
      "source": [
        "# In the following you can define your desired output. Current options:\n",
        "# per_residue embeddings\n",
        "# per_protein embeddings\n",
        "# secondary structure predictions\n",
        "\n",
        "# Replace this file with your own (multi-)FASTA\n",
        "# Headers are expected to start with \">\";\n",
        "seq_path = \"./protT5/example_seqs.fasta\"\n",
        "\n",
        "# whether to retrieve embeddings for each residue in a protein \n",
        "# --> Lx1024 matrix per protein with L being the protein's length\n",
        "# as a rule of thumb: 1k proteins require around 1GB RAM/disk\n",
        "per_residue = True \n",
        "per_residue_path = \"./protT5/output/per_residue_embeddings.h5\" # where to store the embeddings\n",
        "\n",
        "# whether to retrieve per-protein embeddings \n",
        "# --> only one 1024-d vector per protein, irrespective of its length\n",
        "per_protein = False\n",
        "per_protein_path = \"./protT5/output/per_protein_embeddings.h5\" # where to store the embeddings\n",
        "\n",
        "# whether to retrieve secondary structure predictions\n",
        "# This can be replaced by your method after being trained on ProtT5 embeddings\n",
        "sec_struct = False\n",
        "sec_struct_path = \"./protT5/output/ss3_preds.fasta\" # file for storing predictions\n",
        "\n",
        "# make sure that either per-residue or per-protein embeddings are stored\n",
        "assert per_protein is True or per_residue is True or sec_struct is True, print(\n",
        "    \"Minimally, you need to active per_residue, per_protein or sec_struct. (or any combination)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET2v51slC5ui",
        "outputId": "15fa3a38-55dc-4e06-c1e2-5e6783df8456"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pasang/anaconda3/envs/antibody/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@title Import dependencies and check whether GPU is available. { display-mode: \"form\" }\n",
        "from transformers import T5EncoderModel, T5Tokenizer\n",
        "import torch\n",
        "import h5py\n",
        "import time\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using {}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c5XqIyeNStZP"
      },
      "outputs": [],
      "source": [
        "#@title Network architecture for secondary structure prediction. { display-mode: \"form\" }\n",
        "# Convolutional neural network (two convolutional layers) to predict secondary structure\n",
        "class ConvNet( torch.nn.Module ):\n",
        "    def __init__( self ):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # This is only called \"elmo_feature_extractor\" for historic reason\n",
        "        # CNN weights are trained on ProtT5 embeddings\n",
        "        self.elmo_feature_extractor = torch.nn.Sequential(\n",
        "                        torch.nn.Conv2d( 1024, 32, kernel_size=(7,1), padding=(3,0) ), # 7x32\n",
        "                        torch.nn.ReLU(),\n",
        "                        torch.nn.Dropout( 0.25 ),\n",
        "                        )\n",
        "        n_final_in = 32\n",
        "        self.dssp3_classifier = torch.nn.Sequential(\n",
        "                        torch.nn.Conv2d( n_final_in, 3, kernel_size=(7,1), padding=(3,0)) # 7\n",
        "                        )\n",
        "        \n",
        "        self.dssp8_classifier = torch.nn.Sequential(\n",
        "                        torch.nn.Conv2d( n_final_in, 8, kernel_size=(7,1), padding=(3,0))\n",
        "                        )\n",
        "        self.diso_classifier = torch.nn.Sequential(\n",
        "                        torch.nn.Conv2d( n_final_in, 2, kernel_size=(7,1), padding=(3,0))\n",
        "                        )\n",
        "        \n",
        "\n",
        "    def forward( self, x):\n",
        "        # IN: X = (B x L x F); OUT: (B x F x L, 1)\n",
        "        x = x.permute(0,2,1).unsqueeze(dim=-1) \n",
        "        x         = self.elmo_feature_extractor(x) # OUT: (B x 32 x L x 1)\n",
        "        d3_Yhat   = self.dssp3_classifier( x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 3)\n",
        "        d8_Yhat   = self.dssp8_classifier( x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 8)\n",
        "        diso_Yhat = self.diso_classifier(  x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 2)\n",
        "        return d3_Yhat, d8_Yhat, diso_Yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YLYFPAT_VLAK"
      },
      "outputs": [],
      "source": [
        "#@title Load the checkpoint for secondary structure prediction. { display-mode: \"form\" }\n",
        "def load_sec_struct_model():\n",
        "  checkpoint_dir=\"./protT5/sec_struct_checkpoint/secstruct_checkpoint.pt\"\n",
        "  state = torch.load( checkpoint_dir )\n",
        "  model = ConvNet()\n",
        "  model.load_state_dict(state['state_dict'])\n",
        "  model = model.eval()\n",
        "  model = model.to(device)\n",
        "  print('Loaded sec. struct. model from epoch: {:.1f}'.format(state['epoch']))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tLDz6jv1C0UI"
      },
      "outputs": [],
      "source": [
        "#@title Load encoder-part of ProtT5 in half-precision. { display-mode: \"form\" }\n",
        "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50) \n",
        "def get_T5_model():\n",
        "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
        "    model = model.to(device) # move model to GPU\n",
        "    model = model.eval() # set model to evaluation model\n",
        "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6OC1toF1EM9n"
      },
      "outputs": [],
      "source": [
        "#@title Read in file in fasta format. { display-mode: \"form\" }\n",
        "def read_fasta( fasta_path, split_char=\"!\", id_field=0):\n",
        "    '''\n",
        "        Reads in fasta file containing multiple sequences.\n",
        "        Split_char and id_field allow to control identifier extraction from header.\n",
        "        E.g.: set split_char=\"|\" and id_field=1 for SwissProt/UniProt Headers.\n",
        "        Returns dictionary holding multiple sequences or only single \n",
        "        sequence, depending on input file.\n",
        "    '''\n",
        "    \n",
        "    seqs = dict()\n",
        "    with open( fasta_path, 'r' ) as fasta_f:\n",
        "        for line in fasta_f:\n",
        "            # get uniprot ID from header and create new entry\n",
        "            if line.startswith('>'):\n",
        "                uniprot_id = line.replace('>', '').strip().split(split_char)[id_field]\n",
        "                # replace tokens that are mis-interpreted when loading h5\n",
        "                uniprot_id = uniprot_id.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
        "                seqs[ uniprot_id ] = ''\n",
        "            else:\n",
        "                # repl. all whie-space chars and join seqs spanning multiple lines, drop gaps and cast to upper-case\n",
        "                seq= ''.join( line.split() ).upper().replace(\"-\",\"\")\n",
        "                # repl. all non-standard AAs and map them to unknown/X\n",
        "                seq = seq.replace('U','X').replace('Z','X').replace('O','X')\n",
        "                seqs[ uniprot_id ] += seq \n",
        "    example_id=next(iter(seqs))\n",
        "    print(\"Read {} sequences.\".format(len(seqs)))\n",
        "    print(\"Example:\\n{}\\n{}\".format(example_id,seqs[example_id]))\n",
        "\n",
        "    return seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nK4hwGggR_Rs"
      },
      "outputs": [],
      "source": [
        "#@title Generate embeddings. { display-mode: \"form\" }\n",
        "# Generate embeddings via batch-processing\n",
        "# per_residue indicates that embeddings for each residue in a protein should be returned.\n",
        "# per_protein indicates that embeddings for a whole protein should be returned (average-pooling)\n",
        "# max_residues gives the upper limit of residues within one batch\n",
        "# max_seq_len gives the upper sequences length for applying batch-processing\n",
        "# max_batch gives the upper number of sequences per batch\n",
        "def get_embeddings( model, tokenizer, seqs, per_residue, per_protein, sec_struct, \n",
        "                   max_residues=4000, max_seq_len=1000, max_batch=100 ):\n",
        "\n",
        "    if sec_struct:\n",
        "      sec_struct_model = load_sec_struct_model()\n",
        "\n",
        "    results = {\"residue_embs\" : dict(), \n",
        "               \"protein_embs\" : dict(),\n",
        "               \"sec_structs\" : dict() \n",
        "               }\n",
        "\n",
        "    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding)\n",
        "    # seq_dict   = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n",
        "    seq_dict = seqs.items()\n",
        "\n",
        "    start = time.time()\n",
        "    batch = list()\n",
        "    for seq_idx, (pdb_id, seq) in enumerate(seq_dict,1):\n",
        "        seq = seq\n",
        "        seq_len = len(seq)\n",
        "        seq = ' '.join(list(seq))\n",
        "        batch.append((pdb_id,seq,seq_len))\n",
        "\n",
        "        # count residues in current batch and add the last sequence length to\n",
        "        # avoid that batches with (n_res_batch > max_residues) get processed \n",
        "        n_res_batch = sum([ s_len for  _, _, s_len in batch ]) + seq_len \n",
        "        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n",
        "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
        "            print(\"pdb seqs len\",pdb_ids, seqs, seq_lens)\n",
        "            batch = list()\n",
        "\n",
        "            # add_special_tokens adds extra token at the end of each sequence\n",
        "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
        "            input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n",
        "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
        "            \n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
        "                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
        "            except RuntimeError:\n",
        "                print(\"RuntimeError during embedding for {} (L={})\".format(pdb_id, seq_len))\n",
        "                continue\n",
        "\n",
        "            if sec_struct: # in case you want to predict secondary structure from embeddings\n",
        "              d3_Yhat, d8_Yhat, diso_Yhat = sec_struct_model(embedding_repr.last_hidden_state)\n",
        "\n",
        "\n",
        "            for batch_idx, identifier in enumerate(pdb_ids): # for each protein in the current mini-batch\n",
        "                s_len = seq_lens[batch_idx]\n",
        "                # slice off padding --> batch-size x seq_len x embedding_dim  \n",
        "                emb = embedding_repr.last_hidden_state[batch_idx,:s_len]\n",
        "                if sec_struct: # get classification results\n",
        "                    results[\"sec_structs\"][identifier] = torch.max( d3_Yhat[batch_idx,:s_len], dim=1 )[1].detach().cpu().numpy().squeeze()\n",
        "                if per_residue: # store per-residue embeddings (Lx1024)\n",
        "                    results[\"residue_embs\"][ identifier ] = emb.detach().cpu().numpy().squeeze()\n",
        "                if per_protein: # apply average-pooling to derive per-protein embeddings (1024-d)\n",
        "                    protein_emb = emb.mean(dim=0)\n",
        "                    results[\"protein_embs\"][identifier] = protein_emb.detach().cpu().numpy().squeeze()\n",
        "\n",
        "\n",
        "    passed_time=time.time()-start\n",
        "    avg_time = passed_time/len(results[\"residue_embs\"]) if per_residue else passed_time/len(results[\"protein_embs\"])\n",
        "    print('\\n############# EMBEDDING STATS #############')\n",
        "    print('Total number of per-residue embeddings: {}'.format(len(results[\"residue_embs\"])))\n",
        "    print('Total number of per-protein embeddings: {}'.format(len(results[\"protein_embs\"])))\n",
        "    print(\"Time for generating embeddings: {:.1f}[m] ({:.3f}[s/protein])\".format(\n",
        "        passed_time/60, avg_time ))\n",
        "    print('\\n############# END #############')\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UB6yhwunTymY"
      },
      "outputs": [],
      "source": [
        "# #@title Write embeddings to disk. { display-mode: \"form\" }\n",
        "# def save_embeddings(emb_dict,out_path):\n",
        "#     with h5py.File(str(out_path), \"w\") as hf:\n",
        "#         for sequence_id, embedding in emb_dict.items():\n",
        "#             # noinspection PyUnboundLocalVariable\n",
        "#             # hf.create_dataset(sequence_id, data=embedding)\n",
        "#             # Create a dataset with the same shape as the \"embedding\" variable\n",
        "#             dset = hf.create_dataset(sequence_id, shape=embedding.shape, dtype='float32')\n",
        "#             # Assign the \"embedding\" variable to the dataset\n",
        "#             dset[:] = embedding\n",
        "#     return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "def save_embeddings(emb_dict, out_path):\n",
        "    with h5py.File(str(out_path), \"w\") as hf:\n",
        "        group = hf.create_group(\"embeddings\")\n",
        "        for i, (sequence_id, embedding) in enumerate(emb_dict.items()):\n",
        "            # Create a new dataset within the group, using a numerical index as the name of the dataset\n",
        "            dset = group.create_dataset(str(i), data=embedding)\n",
        "            # Create a new attribute for the dataset with the original sequence id name\n",
        "            dset.attrs['sequence_id'] = sequence_id\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "juXcP5Tpbeqv"
      },
      "outputs": [],
      "source": [
        "#@title Write predictions to disk. { display-mode: \"form\" }\n",
        "def write_prediction_fasta(predictions, out_path):\n",
        "  class_mapping = {0:\"H\",1:\"E\",2:\"L\"} \n",
        "  with open(out_path, 'w+') as out_f:\n",
        "      out_f.write( '\\n'.join( \n",
        "          [ \">{}\\n{}\".format( \n",
        "              seq_id, ''.join( [class_mapping[j] for j in yhat] )) \n",
        "          for seq_id, yhat in predictions.items()\n",
        "          ] \n",
        "            ) )\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the encoder part of ProtT5-XL-U50 in half-precision (recommended)\n",
        "model, tokenizer = get_T5_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/pasang/all_experiment/FvFold/data/antibody.h5\n",
            "Keys: <KeysViewHDF5 ['h1_range', 'h2_range', 'h3_range', 'heavy_chain_primary', 'heavy_chain_seq_len', 'id', 'l1_range', 'l2_range', 'l3_range', 'light_chain_primary', 'light_chain_seq_len', 'pairwise_geometry_mat']>\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1baf:H\n",
            "DVQLQESGPGLVKPSQSQSLTCTVTGYSITSDYAWNWIRQFPGNKLEWMGYMSYSGSTRYNPSLRSRISITRDTSKNQFFLQLKSVTTEDTATYFCARGWPLAYWGQGTQVSVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1a6t:H\n",
            "EVQLQQSGPDLVKPGASVKISCKASGYSFSTYYMHWVKQSHGKSLEWIGRVDPDNGGTSFNQKFKGKAILTVDKSSSTAYMELGSLTSEDSAVYYCARRDDYYFDFWGQGTSLTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1ad9:H\n",
            "EIQLVQSGAEVKKPGSSVKVSCKASGYTFTDYYINWMRQAPGQGLEWIGWIDPGSGNTKYNEKFKGRATLTVDTSTNTAYMELSSLRSEDTAFYFCAREKTTYYYAMDYWGQGTLVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1a3r:H\n",
            "EVQLQQSGAELVRPGASVKLSCTTSGFNIKDIYIHWVKQRPEQGLEWIGRLDPANGYTKYDPKFQGKATITVDTSSNTAYLHLSSLTSEDTAVYYCDGYYSYYDMDYWGPGTSVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1afv:H\n",
            "QVQLQQPGSVLVRPGASVKLSCKASGYTFTSSWIHWAKQRPGQGLEWIGEIHPNSGNTNYNEKFKGKATLTVDTSSSTAYVDLSSLTSEDSAVYYCARWRYGSPYYFDYWGQGTTLTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1a5f:H\n",
            "EVALQQSGAELVKPGASVKLSCAASGFTIKDAYMHWVKQKPEQGLEWIGRIDSGSSNTNYDPTFKGKATITADDSSNTAYLQMSSLTSEDTAVYYCARVGLSYWYAMDYWGQGTSVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1adq:H\n",
            "EVQLVESGGGLVQPGRSLRLSCVTSGFTFDDYAMHWVRQSPGKGLEWVSGISWNTGTIIYADSVKGRFIISRDNAKNSLYLQMNSLRVEDTALYYCAKTRSYVVAAEYYFHYWGQGILVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1ay1:H\n",
            "EVQLQESGPGLVKPYQSLSLSCTVTGYSITSDYAWNWIRQFPGNKLEWMGYITYSGTTDYNPSLKSRISITRDTSKNQFFLQLNSVTTEDTATYYCARYYYGYWYFDVWGQGTTLTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1axs:H\n",
            "QVQLLESGAELMKPGASVKISCKATGYTFSSFWIEWVKQRPGHGLEWIGEILPGSGGTHYNEKFKGKATFTADKSSNTAYMQLSSLTSEDSAVYYCARGHSYYFYDGDYWGQGTSVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1a7p:H\n",
            "QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQPPGKGLEWLGMIWGDGNTDYNSALKSRLSISKDNSKSQVFLKMNSLHTDDTARYYCARERDYRLDYWGQGTTVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1ae6:H\n",
            "QIQLQQSGPELVKPGASVKISCKASGYTFTDYYINWMKQKPGQGLEWIGWIDPGSGNTKYNEKFKGKATLTVDTSSSTAYMQLSSLTSEDTAVYFCAREKTTYYYAMDYWGQGTSVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1ahw:H\n",
            "EIQLQQSGAELVRPGALVKLSCKASGFNIKDYYMHWVKQRPEQGLEWIGLIDPENGNTIYDPKFQGKASITADTSSNTAYLQLSSLTSEDTAVYYCARDNSYYFDYWGQGTTLTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1b4j:H\n",
            "QVQLQQPGADLVMPGAPVKLSCLASGYIFTSSWINWVKQRPGRGLEWIGRIDPSDGEVHYNQDFKDKATLTVDKSSSTAYIQLNSLTSEDSAVYYCARGFLPWFADWGQGTLVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1a7q:H\n",
            "QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQLPGKGLEWLGMIWGDGNTAYNSALKSRLSISKDNSKSQVFLEMDSLHTDDTARYYCARERDYRLDYWGQGTTVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1a6v:H\n",
            "QVQLQQPGAELVKPGASVKLSCKASGYTFTSYWMHWVKQRPGRGLEWIGRIDPNSGGTKYNEKFKSKATLTVDKPSSTAYMQLSSLTSEDSAVYYCARYDYYGSSYFDYWGQGTTVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1aj7:H\n",
            "QVQLQQSGAELVKPGASVKLSCTASGFNIKDTYMHWVKQRPEQGLEWIGRIDPANGNTKYDPKFQGKATITADTSSNTAYLQLSSLTSEDTAVYYCASYYGIYWGQGTTLTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1aqk:H\n",
            "QVQLVESGGGVVQPGRSLRLSCAASGFTFNNYAIHWVRQAPGKGLEWVAFISYDGSKNYYADSVKGRFTISRDNSKNTLFLQMNSLRPEDTAIYYCARVLFQQLVLYAPFDIWGQGTMVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1b2w:H\n",
            "QVQLVQSGGGVVQPGRSLKLSCLASGYIFTSSWINWVKQRPGRGLEWIGRIDPSDGEVHYNQDFKDRFTISRDKSKNTLYLQMNSLRPEDTAVYYCARGFLPWFADWGQGTLVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1ai1:H\n",
            "QVKLQESGPAVIKPSQSLSLTCIVSGFSITRTNYCWHWIRQAPGKGLEWMGRICYEGSIYYSPSIKSRSTISRDTSLNKFFIQLISVTNEDTAMYYCSRENHMYETYFDVWGQGTTVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1bfo:H\n",
            "EVKLLESGGGLVQPGGSMRLSCAGSGFTFTDFYMNWIRQPAGKAPEWLGFIRDKAKGYTTEYNPSVKGRFTISRDNTQNMLYLQMNTLRAEDTATYYCAREGHTAAPFDYWGQGVMVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1ad0:H\n",
            "EVQLLESGGGLVQPGGSLRLSCATSGFTFTDYYMNWVRQAPGKGLEWLGFIGNKANGYTTEYSASVKGRFTISRDKSKSTLYLQMNTLQAEDSAIYYCTRDRGLRFYFDYWGQGTLVTVS\n",
            "Read 2 sequences.\n",
            "Example:\n",
            "1a4j:H\n",
            "QVQLLESGPELKKPGETVKISCKASGYTFTNYGMNWVKQAPGKGLKWMGWINTYTGEPTYADDFKGRFAFSLETSASTAYLQINNLKNEDTATYFCVQAERLRRTFDYWGAGTTVTVS\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'1baf:H': 'DVQLQESGPGLVKPSQSQSLTCTVTGYSITSDYAWNWIRQFPGNKLEWMGYMSYSGSTRYNPSLRSRISITRDTSKNQFFLQLKSVTTEDTATYFCARGWPLAYWGQGTQVSVS',\n",
              " '1baf:L': 'QIVLTQSPAIMSASPGEKVTMTCSASSSVYYMYWYQQKPGSSPRLLIYDTSNLASGVPVRFSGSGSGTSYSLTISRMEAEDAATYYCQQWSSYPPITFGVGTKLELKRA',\n",
              " '1a6t:H': 'EVQLQQSGPDLVKPGASVKISCKASGYSFSTYYMHWVKQSHGKSLEWIGRVDPDNGGTSFNQKFKGKAILTVDKSSSTAYMELGSLTSEDSAVYYCARRDDYYFDFWGQGTSLTVS',\n",
              " '1a6t:L': 'QSVLSQSPAILSASPGEKVIMTCSPSSSVSYMQWYQQKPGSSPKPWIYSTSNLASGVPGRFSGGGSGTSFSLTISGVEAEDAATYYCQQYSSHPLTFGGGTKLELKRA',\n",
              " '1ad9:H': 'EIQLVQSGAEVKKPGSSVKVSCKASGYTFTDYYINWMRQAPGQGLEWIGWIDPGSGNTKYNEKFKGRATLTVDTSTNTAYMELSSLRSEDTAFYFCAREKTTYYYAMDYWGQGTLVTVS',\n",
              " '1ad9:L': 'DIQMTQSPSTLSASVGDRVTITCRSSKSLLHSNGDTFLYWFQQKPGKAPKLLMYRMSNLASGVPSRFSGSGSGTEFTLTISSLQPDDFATYYCMQHLEYPFTFGQGTKVEVKRT',\n",
              " '1a3r:H': 'EVQLQQSGAELVRPGASVKLSCTTSGFNIKDIYIHWVKQRPEQGLEWIGRLDPANGYTKYDPKFQGKATITVDTSSNTAYLHLSSLTSEDTAVYYCDGYYSYYDMDYWGPGTSVTVS',\n",
              " '1a3r:L': 'DIVMTQSPSSLTVTTGEKVTMTCKSSQSLLNSRTQKNYLTWYQQKPGQSPKLLIYWASTRESGVPDRFTGSGSGTDFTLSISGVQAEDLAVYYCQNNYNYPLTFGAGTKLELKRA',\n",
              " '1afv:H': 'QVQLQQPGSVLVRPGASVKLSCKASGYTFTSSWIHWAKQRPGQGLEWIGEIHPNSGNTNYNEKFKGKATLTVDTSSSTAYVDLSSLTSEDSAVYYCARWRYGSPYYFDYWGQGTTLTVS',\n",
              " '1afv:L': 'DIVLTQSPASLAVSLGQRATISCRASESVDNYGISFMNWFQQKPGQPPKLLIYAASNLGSGVPARFSGSGSGTDFSLNIHPMEEEDTAMYFCQQSKEVPLTFGAGTKVELKRA',\n",
              " '1a5f:H': 'EVALQQSGAELVKPGASVKLSCAASGFTIKDAYMHWVKQKPEQGLEWIGRIDSGSSNTNYDPTFKGKATITADDSSNTAYLQMSSLTSEDTAVYYCARVGLSYWYAMDYWGQGTSVTVS',\n",
              " '1a5f:L': 'DIVMTQSPSSLTVTTGEKVTMTCKSSQSLLNSGAQKNYLTWYQQKPGQSPKLLIYWASTRESGVPDRFTGSGSGTDFTLSISGVQAEDLAVYYCQNNYNYPLTFGAGTKLELKRA',\n",
              " '1adq:H': 'EVQLVESGGGLVQPGRSLRLSCVTSGFTFDDYAMHWVRQSPGKGLEWVSGISWNTGTIIYADSVKGRFIISRDNAKNSLYLQMNSLRVEDTALYYCAKTRSYVVAAEYYFHYWGQGILVTVS',\n",
              " '1adq:L': 'YVLTQPPSVSVAPGQTARITCGGNNIGSKSVHWYQQKPGQAPVLVVYDDSDRPPGIPERFSGSNSGNTATLTISRVEAGDEADYYCQVWDSSSDHAVFGGGTKLTVLGQP',\n",
              " '1ay1:H': 'EVQLQESGPGLVKPYQSLSLSCTVTGYSITSDYAWNWIRQFPGNKLEWMGYITYSGTTDYNPSLKSRISITRDTSKNQFFLQLNSVTTEDTATYYCARYYYGYWYFDVWGQGTTLTVS',\n",
              " '1ay1:L': 'DIQMTQSPAIMSASPGEKVTMTCSASSSVSYMYWYQQKPGSSPRLLIYDSTNLASGVPVRFSGSGSGTSYSLTISRMEAEDAATYYCQQWSTYPLTFGAGTKLELKRA',\n",
              " '1axs:H': 'QVQLLESGAELMKPGASVKISCKATGYTFSSFWIEWVKQRPGHGLEWIGEILPGSGGTHYNEKFKGKATFTADKSSNTAYMQLSSLTSEDSAVYYCARGHSYYFYDGDYWGQGTSVTVS',\n",
              " '1axs:L': 'ELVLTQSPSSMYASLGERVTITCKASQDINSYLNWFQQKPGKSPKTLIYRTNRLVDGVPSRFSGSGSGQDYSLTISSLEYEDMGIYYCLQYDEFPYTFGSGTKLEIKRT',\n",
              " '1a7p:H': 'QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQPPGKGLEWLGMIWGDGNTDYNSALKSRLSISKDNSKSQVFLKMNSLHTDDTARYYCARERDYRLDYWGQGTTVTVS',\n",
              " '1a7p:L': 'DIVLTQSPASLSASVGETVTITCRASGNIHNYLAWYQQKQGKSPQLLVYYTTTLADGVPSRFSGSGSGTQYSLKINSLQPDDFGSYYCQHFWSTSRTFGGGTKLEIK',\n",
              " '1ae6:H': 'QIQLQQSGPELVKPGASVKISCKASGYTFTDYYINWMKQKPGQGLEWIGWIDPGSGNTKYNEKFKGKATLTVDTSSSTAYMQLSSLTSEDTAVYFCAREKTTYYYAMDYWGQGTSVTVS',\n",
              " '1ae6:L': 'DIVMTQAAPSVPVTPGESLSISCRSSKSLLHSNGDTFLYWFLQRPGQSPQLLIYRMSNLASGVPDRFSGSGSGTAFTLRVSRVEAEDVGVYYCMQHLEYPFTFGAGTKLELKRA',\n",
              " '1ahw:H': 'EIQLQQSGAELVRPGALVKLSCKASGFNIKDYYMHWVKQRPEQGLEWIGLIDPENGNTIYDPKFQGKASITADTSSNTAYLQLSSLTSEDTAVYYCARDNSYYFDYWGQGTTLTVS',\n",
              " '1ahw:L': 'DIKMTQSPSSMYASLGERVTITCKASQDIRKYLNWYQQKPWKSPKTLIYYATSLADGVPSRFSGSGSGQDYSLTISSLESDDTATYYCLQHGESPYTFGGGTKLEINRA',\n",
              " '1b4j:H': 'QVQLQQPGADLVMPGAPVKLSCLASGYIFTSSWINWVKQRPGRGLEWIGRIDPSDGEVHYNQDFKDKATLTVDKSSSTAYIQLNSLTSEDSAVYYCARGFLPWFADWGQGTLVTVS',\n",
              " '1b4j:L': 'NIVMTQSPKSMYVSIGERVTLSCKASENVDTYVSWYQQKPEQSPKLLIYGASNRYTGVPDRFTGSGSATDFTLTISSVQAEDLADYHCGQSYNYPFTFGSGTKLEIKRT',\n",
              " '1a7q:H': 'QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQLPGKGLEWLGMIWGDGNTAYNSALKSRLSISKDNSKSQVFLEMDSLHTDDTARYYCARERDYRLDYWGQGTTVTVS',\n",
              " '1a7q:L': 'DIVLTQSPASLSASVGETVTITCRAGGNTHNYLAWYQQKQGKSPQLLVYYTTTLAAGVPSRFSGSGSGTQYSLKINSLQPDDFGSYYCQHFWSTPRSFGGGTKLEI',\n",
              " '1a6v:H': 'QVQLQQPGAELVKPGASVKLSCKASGYTFTSYWMHWVKQRPGRGLEWIGRIDPNSGGTKYNEKFKSKATLTVDKPSSTAYMQLSSLTSEDSAVYYCARYDYYGSSYFDYWGQGTTVTVS',\n",
              " '1a6v:L': 'QAVVTQESALTTSPGETVTLTCRSSTGAVTTSNYANWVQEKPDHLFTGLIGGTNNRAPGVPARFSGSLIGNKAALTITGAQTEDEAIYFCALWYSNHWVFGGGTKLTVL',\n",
              " '1aj7:H': 'QVQLQQSGAELVKPGASVKLSCTASGFNIKDTYMHWVKQRPEQGLEWIGRIDPANGNTKYDPKFQGKATITADTSSNTAYLQLSSLTSEDTAVYYCASYYGIYWGQGTTLTVS',\n",
              " '1aj7:L': 'DIQMTQSPSSLSASLGERVSLTCRASQEISGYLSWLQQKPDGTIKRLIYAASTLDSGVPKRFSGSRSGSDYSLTISSLESEDFADYYCLQYASYPRTFGGGTKVEIKRT',\n",
              " '1aqk:H': 'QVQLVESGGGVVQPGRSLRLSCAASGFTFNNYAIHWVRQAPGKGLEWVAFISYDGSKNYYADSVKGRFTISRDNSKNTLFLQMNSLRPEDTAIYYCARVLFQQLVLYAPFDIWGQGTMVTVS',\n",
              " '1aqk:L': 'QNVLTQPPSVSGAPGQRVTISCTGSNSNIGAGFTVHWYQHLPGTAPKLLIFANTNRPSGVPDRFSGSKSGTSASLAITGLQAEDEADYYCQSYDSSLSARFGGGTRLTVLGQP',\n",
              " '1b2w:H': 'QVQLVQSGGGVVQPGRSLKLSCLASGYIFTSSWINWVKQRPGRGLEWIGRIDPSDGEVHYNQDFKDRFTISRDKSKNTLYLQMNSLRPEDTAVYYCARGFLPWFADWGQGTLVTVS',\n",
              " '1b2w:L': 'DIQMTQSPSTLSASVGDRVTITCKASENVDTYVSWYQQKPGKAPKLLIYGASNRYTGVPSRFSGSGSGTDFTLTISSLQPDDFATYYCGQSYNYPFTFGQGTKVEVKRT',\n",
              " '1ai1:H': 'QVKLQESGPAVIKPSQSLSLTCIVSGFSITRTNYCWHWIRQAPGKGLEWMGRICYEGSIYYSPSIKSRSTISRDTSLNKFFIQLISVTNEDTAMYYCSRENHMYETYFDVWGQGTTVTVS',\n",
              " '1ai1:L': 'DIVMTQSPASLVVSLGQRATISCRASESVDSYGKSFMHWYQQKPGQPPKVLIYIASNLESGVPARFSGSGSRTDFTLTIDPVEADDAATYYCQQNNEDPPTFGAGTKLEMRRA',\n",
              " '1bfo:H': 'EVKLLESGGGLVQPGGSMRLSCAGSGFTFTDFYMNWIRQPAGKAPEWLGFIRDKAKGYTTEYNPSVKGRFTISRDNTQNMLYLQMNTLRAEDTATYYCAREGHTAAPFDYWGQGVMVTVS',\n",
              " '1bfo:L': 'DIKMTQSPSFLSASVGDRVTLNCKASQNIDKYLNWYQQKLGESPKLLIYNTNNLQTGIPSRFSGSGSGTDFTLTISSLQPEDVATYFCLQHISRPRTFGTGTKLELKRA',\n",
              " '1ad0:H': 'EVQLLESGGGLVQPGGSLRLSCATSGFTFTDYYMNWVRQAPGKGLEWLGFIGNKANGYTTEYSASVKGRFTISRDKSKSTLYLQMNTLQAEDSAIYYCTRDRGLRFYFDYWGQGTLVTVS',\n",
              " '1ad0:L': 'QTVLTQSPSSLSVSVGDRVTITCRASSSVTYIHWYQQKPGLAPKSLIYATSNLASGVPSRFSGSGSGTDYTFTISSLQPEDIATYYCQHWSSKPPTFGQGTKVEVKRT',\n",
              " '1a4j:H': 'QVQLLESGPELKKPGETVKISCKASGYTFTNYGMNWVKQAPGKGLKWMGWINTYTGEPTYADDFKGRFAFSLETSASTAYLQINNLKNEDTATYFCVQAERLRRTFDYWGAGTTVTVS',\n",
              " '1a4j:L': 'ELVMTQTPLSLPVSLGDQASISCRSSQSLVHSNGNTYLHWYLQKPGQSPKLLIYKVSNRFSGVPDRFSGSGSGTDFTLKISRVEAEDLGVYFCSQSTHVPPTFGGGTKLEIKRT'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from read_fasta import *\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/home/pasang/all_experiment/FvFold')\n",
        "import fvfold\n",
        "project_path = os.path.abspath(os.path.join(fvfold.__file__, \"../..\"))\n",
        "path = os.path.join(project_path, \"data/\")\n",
        "filename=\"antibody.h5\"\n",
        "fasta_dir=os.path.join(path, \"antibody_database/\")\n",
        "all_seq=read_all_seq(path,filename,fasta_dir)\n",
        "all_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UKa6VWeE6kC",
        "outputId": "f9bbc5bd-2ab4-4ef5-a1f0-73ccddb2c722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pdb seqs len ('1baf:H', '1baf:L', '1a6t:H', '1a6t:L', '1ad9:H', '1ad9:L', '1a3r:H', '1a3r:L', '1afv:H', '1afv:L', '1a5f:H', '1a5f:L', '1adq:H', '1adq:L', '1ay1:H', '1ay1:L', '1axs:H', '1axs:L', '1a7p:H', '1a7p:L', '1ae6:H', '1ae6:L', '1ahw:H', '1ahw:L', '1b4j:H', '1b4j:L', '1a7q:H', '1a7q:L', '1a6v:H', '1a6v:L', '1aj7:H', '1aj7:L', '1aqk:H', '1aqk:L', '1b2w:H') ('D V Q L Q E S G P G L V K P S Q S Q S L T C T V T G Y S I T S D Y A W N W I R Q F P G N K L E W M G Y M S Y S G S T R Y N P S L R S R I S I T R D T S K N Q F F L Q L K S V T T E D T A T Y F C A R G W P L A Y W G Q G T Q V S V S', 'Q I V L T Q S P A I M S A S P G E K V T M T C S A S S S V Y Y M Y W Y Q Q K P G S S P R L L I Y D T S N L A S G V P V R F S G S G S G T S Y S L T I S R M E A E D A A T Y Y C Q Q W S S Y P P I T F G V G T K L E L K R A', 'E V Q L Q Q S G P D L V K P G A S V K I S C K A S G Y S F S T Y Y M H W V K Q S H G K S L E W I G R V D P D N G G T S F N Q K F K G K A I L T V D K S S S T A Y M E L G S L T S E D S A V Y Y C A R R D D Y Y F D F W G Q G T S L T V S', 'Q S V L S Q S P A I L S A S P G E K V I M T C S P S S S V S Y M Q W Y Q Q K P G S S P K P W I Y S T S N L A S G V P G R F S G G G S G T S F S L T I S G V E A E D A A T Y Y C Q Q Y S S H P L T F G G G T K L E L K R A', 'E I Q L V Q S G A E V K K P G S S V K V S C K A S G Y T F T D Y Y I N W M R Q A P G Q G L E W I G W I D P G S G N T K Y N E K F K G R A T L T V D T S T N T A Y M E L S S L R S E D T A F Y F C A R E K T T Y Y Y A M D Y W G Q G T L V T V S', 'D I Q M T Q S P S T L S A S V G D R V T I T C R S S K S L L H S N G D T F L Y W F Q Q K P G K A P K L L M Y R M S N L A S G V P S R F S G S G S G T E F T L T I S S L Q P D D F A T Y Y C M Q H L E Y P F T F G Q G T K V E V K R T', 'E V Q L Q Q S G A E L V R P G A S V K L S C T T S G F N I K D I Y I H W V K Q R P E Q G L E W I G R L D P A N G Y T K Y D P K F Q G K A T I T V D T S S N T A Y L H L S S L T S E D T A V Y Y C D G Y Y S Y Y D M D Y W G P G T S V T V S', 'D I V M T Q S P S S L T V T T G E K V T M T C K S S Q S L L N S R T Q K N Y L T W Y Q Q K P G Q S P K L L I Y W A S T R E S G V P D R F T G S G S G T D F T L S I S G V Q A E D L A V Y Y C Q N N Y N Y P L T F G A G T K L E L K R A', 'Q V Q L Q Q P G S V L V R P G A S V K L S C K A S G Y T F T S S W I H W A K Q R P G Q G L E W I G E I H P N S G N T N Y N E K F K G K A T L T V D T S S S T A Y V D L S S L T S E D S A V Y Y C A R W R Y G S P Y Y F D Y W G Q G T T L T V S', 'D I V L T Q S P A S L A V S L G Q R A T I S C R A S E S V D N Y G I S F M N W F Q Q K P G Q P P K L L I Y A A S N L G S G V P A R F S G S G S G T D F S L N I H P M E E E D T A M Y F C Q Q S K E V P L T F G A G T K V E L K R A', 'E V A L Q Q S G A E L V K P G A S V K L S C A A S G F T I K D A Y M H W V K Q K P E Q G L E W I G R I D S G S S N T N Y D P T F K G K A T I T A D D S S N T A Y L Q M S S L T S E D T A V Y Y C A R V G L S Y W Y A M D Y W G Q G T S V T V S', 'D I V M T Q S P S S L T V T T G E K V T M T C K S S Q S L L N S G A Q K N Y L T W Y Q Q K P G Q S P K L L I Y W A S T R E S G V P D R F T G S G S G T D F T L S I S G V Q A E D L A V Y Y C Q N N Y N Y P L T F G A G T K L E L K R A', 'E V Q L V E S G G G L V Q P G R S L R L S C V T S G F T F D D Y A M H W V R Q S P G K G L E W V S G I S W N T G T I I Y A D S V K G R F I I S R D N A K N S L Y L Q M N S L R V E D T A L Y Y C A K T R S Y V V A A E Y Y F H Y W G Q G I L V T V S', 'Y V L T Q P P S V S V A P G Q T A R I T C G G N N I G S K S V H W Y Q Q K P G Q A P V L V V Y D D S D R P P G I P E R F S G S N S G N T A T L T I S R V E A G D E A D Y Y C Q V W D S S S D H A V F G G G T K L T V L G Q P', 'E V Q L Q E S G P G L V K P Y Q S L S L S C T V T G Y S I T S D Y A W N W I R Q F P G N K L E W M G Y I T Y S G T T D Y N P S L K S R I S I T R D T S K N Q F F L Q L N S V T T E D T A T Y Y C A R Y Y Y G Y W Y F D V W G Q G T T L T V S', 'D I Q M T Q S P A I M S A S P G E K V T M T C S A S S S V S Y M Y W Y Q Q K P G S S P R L L I Y D S T N L A S G V P V R F S G S G S G T S Y S L T I S R M E A E D A A T Y Y C Q Q W S T Y P L T F G A G T K L E L K R A', 'Q V Q L L E S G A E L M K P G A S V K I S C K A T G Y T F S S F W I E W V K Q R P G H G L E W I G E I L P G S G G T H Y N E K F K G K A T F T A D K S S N T A Y M Q L S S L T S E D S A V Y Y C A R G H S Y Y F Y D G D Y W G Q G T S V T V S', 'E L V L T Q S P S S M Y A S L G E R V T I T C K A S Q D I N S Y L N W F Q Q K P G K S P K T L I Y R T N R L V D G V P S R F S G S G S G Q D Y S L T I S S L E Y E D M G I Y Y C L Q Y D E F P Y T F G S G T K L E I K R T', 'Q V Q L Q E S G P G L V A P S Q S L S I T C T V S G F S L T G Y G V N W V R Q P P G K G L E W L G M I W G D G N T D Y N S A L K S R L S I S K D N S K S Q V F L K M N S L H T D D T A R Y Y C A R E R D Y R L D Y W G Q G T T V T V S', 'D I V L T Q S P A S L S A S V G E T V T I T C R A S G N I H N Y L A W Y Q Q K Q G K S P Q L L V Y Y T T T L A D G V P S R F S G S G S G T Q Y S L K I N S L Q P D D F G S Y Y C Q H F W S T S R T F G G G T K L E I K', 'Q I Q L Q Q S G P E L V K P G A S V K I S C K A S G Y T F T D Y Y I N W M K Q K P G Q G L E W I G W I D P G S G N T K Y N E K F K G K A T L T V D T S S S T A Y M Q L S S L T S E D T A V Y F C A R E K T T Y Y Y A M D Y W G Q G T S V T V S', 'D I V M T Q A A P S V P V T P G E S L S I S C R S S K S L L H S N G D T F L Y W F L Q R P G Q S P Q L L I Y R M S N L A S G V P D R F S G S G S G T A F T L R V S R V E A E D V G V Y Y C M Q H L E Y P F T F G A G T K L E L K R A', 'E I Q L Q Q S G A E L V R P G A L V K L S C K A S G F N I K D Y Y M H W V K Q R P E Q G L E W I G L I D P E N G N T I Y D P K F Q G K A S I T A D T S S N T A Y L Q L S S L T S E D T A V Y Y C A R D N S Y Y F D Y W G Q G T T L T V S', 'D I K M T Q S P S S M Y A S L G E R V T I T C K A S Q D I R K Y L N W Y Q Q K P W K S P K T L I Y Y A T S L A D G V P S R F S G S G S G Q D Y S L T I S S L E S D D T A T Y Y C L Q H G E S P Y T F G G G T K L E I N R A', 'Q V Q L Q Q P G A D L V M P G A P V K L S C L A S G Y I F T S S W I N W V K Q R P G R G L E W I G R I D P S D G E V H Y N Q D F K D K A T L T V D K S S S T A Y I Q L N S L T S E D S A V Y Y C A R G F L P W F A D W G Q G T L V T V S', 'N I V M T Q S P K S M Y V S I G E R V T L S C K A S E N V D T Y V S W Y Q Q K P E Q S P K L L I Y G A S N R Y T G V P D R F T G S G S A T D F T L T I S S V Q A E D L A D Y H C G Q S Y N Y P F T F G S G T K L E I K R T', 'Q V Q L Q E S G P G L V A P S Q S L S I T C T V S G F S L T G Y G V N W V R Q L P G K G L E W L G M I W G D G N T A Y N S A L K S R L S I S K D N S K S Q V F L E M D S L H T D D T A R Y Y C A R E R D Y R L D Y W G Q G T T V T V S', 'D I V L T Q S P A S L S A S V G E T V T I T C R A G G N T H N Y L A W Y Q Q K Q G K S P Q L L V Y Y T T T L A A G V P S R F S G S G S G T Q Y S L K I N S L Q P D D F G S Y Y C Q H F W S T P R S F G G G T K L E I', 'Q V Q L Q Q P G A E L V K P G A S V K L S C K A S G Y T F T S Y W M H W V K Q R P G R G L E W I G R I D P N S G G T K Y N E K F K S K A T L T V D K P S S T A Y M Q L S S L T S E D S A V Y Y C A R Y D Y Y G S S Y F D Y W G Q G T T V T V S', 'Q A V V T Q E S A L T T S P G E T V T L T C R S S T G A V T T S N Y A N W V Q E K P D H L F T G L I G G T N N R A P G V P A R F S G S L I G N K A A L T I T G A Q T E D E A I Y F C A L W Y S N H W V F G G G T K L T V L', 'Q V Q L Q Q S G A E L V K P G A S V K L S C T A S G F N I K D T Y M H W V K Q R P E Q G L E W I G R I D P A N G N T K Y D P K F Q G K A T I T A D T S S N T A Y L Q L S S L T S E D T A V Y Y C A S Y Y G I Y W G Q G T T L T V S', 'D I Q M T Q S P S S L S A S L G E R V S L T C R A S Q E I S G Y L S W L Q Q K P D G T I K R L I Y A A S T L D S G V P K R F S G S R S G S D Y S L T I S S L E S E D F A D Y Y C L Q Y A S Y P R T F G G G T K V E I K R T', 'Q V Q L V E S G G G V V Q P G R S L R L S C A A S G F T F N N Y A I H W V R Q A P G K G L E W V A F I S Y D G S K N Y Y A D S V K G R F T I S R D N S K N T L F L Q M N S L R P E D T A I Y Y C A R V L F Q Q L V L Y A P F D I W G Q G T M V T V S', 'Q N V L T Q P P S V S G A P G Q R V T I S C T G S N S N I G A G F T V H W Y Q H L P G T A P K L L I F A N T N R P S G V P D R F S G S K S G T S A S L A I T G L Q A E D E A D Y Y C Q S Y D S S L S A R F G G G T R L T V L G Q P', 'Q V Q L V Q S G G G V V Q P G R S L K L S C L A S G Y I F T S S W I N W V K Q R P G R G L E W I G R I D P S D G E V H Y N Q D F K D R F T I S R D K S K N T L Y L Q M N S L R P E D T A V Y Y C A R G F L P W F A D W G Q G T L V T V S') (114, 109, 116, 108, 119, 114, 117, 115, 119, 113, 119, 115, 122, 110, 118, 108, 119, 109, 115, 107, 119, 114, 116, 109, 116, 109, 115, 106, 119, 109, 113, 109, 122, 113, 116)\n",
            "pdb seqs len ('1b2w:L', '1ai1:H', '1ai1:L', '1bfo:H', '1bfo:L', '1ad0:H', '1ad0:L', '1a4j:H', '1a4j:L') ('D I Q M T Q S P S T L S A S V G D R V T I T C K A S E N V D T Y V S W Y Q Q K P G K A P K L L I Y G A S N R Y T G V P S R F S G S G S G T D F T L T I S S L Q P D D F A T Y Y C G Q S Y N Y P F T F G Q G T K V E V K R T', 'Q V K L Q E S G P A V I K P S Q S L S L T C I V S G F S I T R T N Y C W H W I R Q A P G K G L E W M G R I C Y E G S I Y Y S P S I K S R S T I S R D T S L N K F F I Q L I S V T N E D T A M Y Y C S R E N H M Y E T Y F D V W G Q G T T V T V S', 'D I V M T Q S P A S L V V S L G Q R A T I S C R A S E S V D S Y G K S F M H W Y Q Q K P G Q P P K V L I Y I A S N L E S G V P A R F S G S G S R T D F T L T I D P V E A D D A A T Y Y C Q Q N N E D P P T F G A G T K L E M R R A', 'E V K L L E S G G G L V Q P G G S M R L S C A G S G F T F T D F Y M N W I R Q P A G K A P E W L G F I R D K A K G Y T T E Y N P S V K G R F T I S R D N T Q N M L Y L Q M N T L R A E D T A T Y Y C A R E G H T A A P F D Y W G Q G V M V T V S', 'D I K M T Q S P S F L S A S V G D R V T L N C K A S Q N I D K Y L N W Y Q Q K L G E S P K L L I Y N T N N L Q T G I P S R F S G S G S G T D F T L T I S S L Q P E D V A T Y F C L Q H I S R P R T F G T G T K L E L K R A', 'E V Q L L E S G G G L V Q P G G S L R L S C A T S G F T F T D Y Y M N W V R Q A P G K G L E W L G F I G N K A N G Y T T E Y S A S V K G R F T I S R D K S K S T L Y L Q M N T L Q A E D S A I Y Y C T R D R G L R F Y F D Y W G Q G T L V T V S', 'Q T V L T Q S P S S L S V S V G D R V T I T C R A S S S V T Y I H W Y Q Q K P G L A P K S L I Y A T S N L A S G V P S R F S G S G S G T D Y T F T I S S L Q P E D I A T Y Y C Q H W S S K P P T F G Q G T K V E V K R T', 'Q V Q L L E S G P E L K K P G E T V K I S C K A S G Y T F T N Y G M N W V K Q A P G K G L K W M G W I N T Y T G E P T Y A D D F K G R F A F S L E T S A S T A Y L Q I N N L K N E D T A T Y F C V Q A E R L R R T F D Y W G A G T T V T V S', 'E L V M T Q T P L S L P V S L G D Q A S I S C R S S Q S L V H S N G N T Y L H W Y L Q K P G Q S P K L L I Y K V S N R F S G V P D R F S G S G S G T D F T L K I S R V E A E D L G V Y F C S Q S T H V P P T F G G G T K L E I K R T') (109, 120, 113, 120, 109, 120, 108, 118, 114)\n",
            "\n",
            "############# EMBEDDING STATS #############\n",
            "Total number of per-residue embeddings: 44\n",
            "Total number of per-protein embeddings: 0\n",
            "Time for generating embeddings: 0.0[m] (0.043[s/protein])\n",
            "\n",
            "############# END #############\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load example fasta.\n",
        "# seqs = read_fasta( seq_path )\n",
        "\n",
        "# Compute embeddings and/or secondary structure predictions\n",
        "results = get_embeddings( model, tokenizer, all_seq,\n",
        "                         per_residue, per_protein, sec_struct)\n",
        "per_residue_path =os.path.join(path, \"per_residue_embeddings.h5\")\n",
        "# Store per-residue embeddings\n",
        "if per_residue:\n",
        "  save_embeddings(results[\"residue_embs\"], per_residue_path)\n",
        "# if per_protein:\n",
        "#   save_embeddings(results[\"protein_embs\"], per_protein_path)\n",
        "# if sec_struct:\n",
        "#   write_prediction_fasta(results[\"sec_structs\"], sec_struct_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['1baf:H', '1baf:L', '1a6t:H', '1a6t:L', '1ad9:H', '1ad9:L', '1a3r:H', '1a3r:L', '1afv:H', '1afv:L', '1a5f:H', '1a5f:L', '1adq:H', '1adq:L', '1ay1:H', '1ay1:L', '1axs:H', '1axs:L', '1a7p:H', '1a7p:L', '1ae6:H', '1ae6:L', '1ahw:H', '1ahw:L', '1b4j:H', '1b4j:L', '1a7q:H', '1a7q:L', '1a6v:H', '1a6v:L', '1aj7:H', '1aj7:L', '1aqk:H', '1aqk:L', '1b2w:H', '1b2w:L', '1ai1:H', '1ai1:L', '1bfo:H', '1bfo:L', '1ad0:H', '1ad0:L', '1a4j:H', '1a4j:L'])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def read_embeddings(file_path):\n",
        "    emb_dict = {}\n",
        "    with h5py.File(file_path, 'r') as hf:\n",
        "        group = hf['embeddings']\n",
        "        for key in sorted(group.keys(), key=lambda x: int(x)):\n",
        "            emb_dict[group[key].attrs['sequence_id']] = group[key][:]\n",
        "    return emb_dict\n",
        "emb_dict = read_embeddings(os.path.join(path, \"per_residue_embeddings.h5\"))\n",
        "emb_dict.keys()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "history_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "antibody",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4aa1b9ed64ceffde4e9ec8fae6a09901f619502af945619ec22cc09864d41a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
